{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install fastapi\n",
    "#!pip install statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYKuDfjp2Jxd"
   },
   "source": [
    "Impor Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "IE3efW1p1tMU",
    "outputId": "54d20f93-ce0b-4e34-f9ea-688885875094"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import uuid\n",
    "import time\n",
    "import sqlite3\n",
    "import tempfile\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Tokenization\n",
    "try:\n",
    "    import tiktoken\n",
    "    _TIKTOKEN_AVAILABLE = True\n",
    "except Exception:\n",
    "    _TIKTOKEN_AVAILABLE = False\n",
    "\n",
    "# OpenAI\n",
    "import openai\n",
    "\n",
    "# FAISS\n",
    "import faiss\n",
    "\n",
    "# LangChain + SQLAlchemy + LLM wrappers\n",
    "from sqlalchemy import create_engine, text\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.agents import create_sql_agent\n",
    "from langchain_community.utilities.sql_database import SQLDatabase\n",
    "\n",
    "# FastAPI for demo endpoints\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "\n",
    "# Plotly\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "# Statsmodels\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Load dotenv if present\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrzRwik04T4c"
   },
   "source": [
    "CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Qv6QumpE2oBC"
   },
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", None) \n",
    "if not OPENAI_API_KEY:\n",
    "    raise EnvironmentError(\"Set OPENAI_API_KEY in environment variables\")\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdiwUepY4aaz"
   },
   "source": [
    "LLM config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QAqsHT8E4bAH"
   },
   "outputs": [],
   "source": [
    "LLM_MODEL_NAME = os.getenv(\"LLM_MODEL_NAME\", \"gpt-4o-mini\")  # change to available model\n",
    "EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eID5YOou4i_-"
   },
   "source": [
    "FAISS persist directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sDpSjWRF4fHr"
   },
   "outputs": [],
   "source": [
    "FAISS_INDEX_DIR = Path(\"./faiss_data\")\n",
    "FAISS_INDEX_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47R6ARoz4opF"
   },
   "source": [
    "UTIL: Tokenizer & chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "oeaNeaGp4fKQ"
   },
   "outputs": [],
   "source": [
    "def get_token_count(text: str, model: str = \"gpt-4o-mini\") -> int:\n",
    "    if _TIKTOKEN_AVAILABLE:\n",
    "        enc = tiktoken.encoding_for_model(model)\n",
    "        return len(enc.encode(text))\n",
    "    # fallback approx: 1 token ~ 4 chars\n",
    "    return max(1, len(text) // 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dOU-8kVg4fMx"
   },
   "outputs": [],
   "source": [
    "def chunk_text_token_aware(text: str, min_tokens=300, max_tokens=500, model=\"gpt-4o-mini\") -> List[str]:\n",
    "    \"\"\"\n",
    "    Token-aware chunker: uses tiktoken when available, otherwise a simple word-based fallback.\n",
    "    Keeps overlap to preserve context.\n",
    "    \"\"\"\n",
    "    if _TIKTOKEN_AVAILABLE:\n",
    "        enc = tiktoken.encoding_for_model(model)\n",
    "        toks = enc.encode(text)\n",
    "        chunks = []\n",
    "        i = 0\n",
    "        overlap = int(min_tokens * 0.1)\n",
    "        while i < len(toks):\n",
    "            end = min(i + max_tokens, len(toks))\n",
    "            chunk_toks = toks[i:end]\n",
    "            chunks.append(enc.decode(chunk_toks))\n",
    "            if end == len(toks):\n",
    "                break\n",
    "            i = end - overlap\n",
    "        return chunks\n",
    "    else:\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        i = 0\n",
    "        overlap = max(10, int(min_tokens*0.1))\n",
    "        while i < len(words):\n",
    "            end = min(i + max_tokens, len(words))\n",
    "            chunks.append(\" \".join(words[i:end]))\n",
    "            if end == len(words):\n",
    "                break\n",
    "            i = end - overlap\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHgeybC85UZl"
   },
   "source": [
    "DOC LOADER & METADATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "iRm6i3uV4fPU"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DocChunk:\n",
    "    id: str\n",
    "    doc_id: str\n",
    "    chunk_id: int\n",
    "    text: str\n",
    "    metadata: Dict[str, Any]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "dTCZPlGR4fRr"
   },
   "outputs": [],
   "source": [
    "def load_text_docs_from_folder(folder: Path, pattern: str = \"*.txt\") -> List[Tuple[str,str]]:\n",
    "    docs = []\n",
    "    for f in folder.glob(pattern):\n",
    "        docs.append((str(f), f.read_text(encoding=\"utf-8\")))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "lN-hAUZ-4fUG"
   },
   "outputs": [],
   "source": [
    "def create_chunks_from_doc(doc_id: str, text: str, min_tokens=300, max_tokens=500) -> List[DocChunk]:\n",
    "    raw_chunks = chunk_text_token_aware(text, min_tokens=min_tokens, max_tokens=max_tokens)\n",
    "    chunks = []\n",
    "    for i, c in enumerate(raw_chunks):\n",
    "        chunks.append(DocChunk(\n",
    "            id=str(uuid.uuid4()),\n",
    "            doc_id=doc_id,\n",
    "            chunk_id=i,\n",
    "            text=c,\n",
    "            metadata={\"doc_id\": doc_id, \"chunk_id\": i}\n",
    "        ))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otup5mu54fWf"
   },
   "source": [
    "EMBEDDING CLIENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8mqjslOK4fYz"
   },
   "outputs": [],
   "source": [
    "class EmbeddingClient:\n",
    "    def __init__(self, model_name=EMBEDDING_MODEL):\n",
    "        self.model = model_name\n",
    "\n",
    "    def embed_batch(self, texts: List[str]) -> List[List[float]]:\n",
    "        # OpenAI embeddings\n",
    "        resp = openai.Embedding.create(model=self.model, input=texts)\n",
    "        embeds = [r[\"embedding\"] for r in resp[\"data\"]]\n",
    "        return embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIBdztl64fbT"
   },
   "source": [
    "VECTORSTORE: FAISS wrappe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "VA7ABXcU4fdt"
   },
   "outputs": [],
   "source": [
    "class FaissVectorStore:\n",
    "    def __init__(self, dim: int, persist_dir: Path = FAISS_INDEX_DIR, index_name: str = \"default\"):\n",
    "        self.dim = dim\n",
    "        self.index_name = index_name\n",
    "        self.persist_dir = persist_dir\n",
    "        self.index_path = persist_dir / f\"{index_name}.index\"\n",
    "        self.id_to_meta_path = persist_dir / f\"{index_name}_meta.json\"\n",
    "\n",
    "        # IndexFlatIP + L2-normalized embeddings for cosine similarity\n",
    "        self.index = faiss.IndexFlatIP(dim)\n",
    "        self.ids = []  # parallel list of ids\n",
    "        self.metadict = {}\n",
    "\n",
    "        if self.index_path.exists() and self.id_to_meta_path.exists():\n",
    "            try:\n",
    "                self.load()\n",
    "            except Exception as e:\n",
    "                print(\"Failed to load existing FAISS index:\", e)\n",
    "\n",
    "    def add(self, ids: List[str], vectors: np.ndarray, metadatas: List[dict]):\n",
    "        assert vectors.shape[1] == self.dim\n",
    "        # normalize\n",
    "        faiss.normalize_L2(vectors)\n",
    "        self.index.add(vectors)\n",
    "        self.ids.extend(ids)\n",
    "        for i, _id in enumerate(ids):\n",
    "            self.metadict[_id] = metadatas[i]\n",
    "        self._save()\n",
    "\n",
    "    def search(self, qvec: np.ndarray, top_k=10) -> List[Tuple[str, float]]:\n",
    "        # qvec expected shape (dim,)\n",
    "        q = qvec.reshape(1, -1).astype('float32')\n",
    "        faiss.normalize_L2(q)\n",
    "        dists, idxs = self.index.search(q, top_k)\n",
    "        results = []\n",
    "        for score, idx in zip(dists[0], idxs[0]):\n",
    "            if idx < 0 or idx >= len(self.ids):\n",
    "                continue\n",
    "            results.append((self.ids[idx], float(score)))\n",
    "        return results\n",
    "\n",
    "    def _save(self):\n",
    "        faiss.write_index(self.index, str(self.index_path))\n",
    "        with open(self.id_to_meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.metadict, f)\n",
    "\n",
    "    def load(self):\n",
    "        self.index = faiss.read_index(str(self.index_path))\n",
    "        with open(self.id_to_meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.metadict = json.load(f)\n",
    "        self.ids = list(self.metadict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uH0VeVwg58Xn"
   },
   "source": [
    "RETRIEVER: Dense + Keyword overlap hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "DWqm5kCi4fgC"
   },
   "outputs": [],
   "source": [
    "class HybridRetriever:\n",
    "    def __init__(self, vectorstore: FaissVectorStore, embedding_client: EmbeddingClient):\n",
    "        self.vs = vectorstore\n",
    "        self.emb = embedding_client\n",
    "\n",
    "    def _keyword_score(self, query: str, text: str) -> float:\n",
    "        # cheap token overlap score normalized\n",
    "        qset = set(query.lower().split())\n",
    "        tset = set(text.lower().split())\n",
    "        if not qset:\n",
    "            return 0.0\n",
    "        return len(qset.intersection(tset)) / len(qset)\n",
    "\n",
    "    def retrieve(self, query: str, top_k=5, dense_k=30) -> List[Dict[str, Any]]:\n",
    "        # 1. dense search\n",
    "        q_embed = np.array(self.emb.embed_batch([query]), dtype='float32')\n",
    "        qvec = q_embed[0]\n",
    "        dense_results = self.vs.search(qvec, top_k=dense_k)  # returns (id, score)\n",
    "        # build candidate list\n",
    "        candidates = []\n",
    "        for _id, score in dense_results:\n",
    "            meta = self.vs.metadict.get(_id, {})\n",
    "            text = meta.get(\"text\", \"\")\n",
    "            kw_score = self._keyword_score(query, text)\n",
    "            combined_score = 0.7 * score + 0.3 * kw_score\n",
    "            candidates.append({\"id\": _id, \"score\": combined_score, \"meta\": meta, \"text\": text})\n",
    "        # rank and return top_k\n",
    "        candidates = sorted(candidates, key=lambda x: x[\"score\"], reverse=True)[:top_k]\n",
    "        return candidates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76OKxpVE6G8C"
   },
   "source": [
    "RAG / RetrievalQA wrapper using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "UAwGIlzb4fil"
   },
   "outputs": [],
   "source": [
    "class RAGService:\n",
    "    def __init__(self, retriever: HybridRetriever, llm_model_name=LLM_MODEL_NAME, temperature=0.0):\n",
    "        self.retriever = retriever\n",
    "        # wrap OpenAI chat model via LangChain\n",
    "        self.llm = ChatOpenAI(model=llm_model_name, temperature=temperature)\n",
    "\n",
    "    def answer(self, query: str, top_k=5) -> Dict[str, Any]:\n",
    "        docs = self.retriever.retrieve(query, top_k=top_k)\n",
    "        # convert to langchain Documents\n",
    "        lc_docs = []\n",
    "        for d in docs:\n",
    "            metadata = d[\"meta\"].copy()\n",
    "            # include provenance\n",
    "            provenance = {\"id\": d[\"id\"], \"score\": d[\"score\"]}\n",
    "            metadata[\"provenance\"] = provenance\n",
    "            lc_docs.append(Document(page_content=d[\"text\"], metadata=metadata))\n",
    "        # Build a simple RetrievalQA chain\n",
    "        # For large scale: use map_rerank or custom chain\n",
    "        qa = RetrievalQA.from_chain_type(llm=self.llm, chain_type=\"map_rerank\", retriever=None)\n",
    "        # Since we don't use a langchain vectorstore retriever here, we directly run LLM with docs as context\n",
    "        # Compose a prompt manually: system + context + user query\n",
    "        context_text = \"\\n\\n---\\n\\n\".join([f\"[{doc.metadata.get('provenance')}] {doc.page_content}\" for doc in lc_docs])\n",
    "        system_prompt = (\n",
    "            \"You are an expert data scientist and knowledge assistant. Use the provided context strictly, \"\n",
    "            \"cite provenance in square brackets (doc_id,chunk_id). Answer concisely and show steps if applicable.\"\n",
    "        )\n",
    "        user_prompt = f\"Context:\\n{context_text}\\n\\nUser question: {query}\\n\\nAnswer with provenance.\"\n",
    "        resp = self.llm.predict_messages([{\"role\":\"system\",\"content\":system_prompt}, {\"role\":\"user\",\"content\":user_prompt}])\n",
    "        return {\"answer\": resp.content, \"provenance\": [doc.metadata.get(\"provenance\") for doc in lc_docs], \"docs\": [asdict(d) for d in docs]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "furuXzcN6O8n"
   },
   "source": [
    "SQL Agent & DB setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "nujB9R0G4fk7"
   },
   "outputs": [],
   "source": [
    "class SQLAgentService:\n",
    "    def __init__(self, db_engine):\n",
    "        self.engine = db_engine\n",
    "        self.db = SQLDatabase(self.engine)\n",
    "        self.llm = ChatOpenAI(model=LLM_MODEL_NAME, temperature=0)\n",
    "\n",
    "        # create agent\n",
    "        self.agent = create_sql_agent(self.llm, self.db, verbose=False)\n",
    "\n",
    "    def run_nl_query(self, nl: str) -> str:\n",
    "        # produce SQL & run it via agent\n",
    "        try:\n",
    "            res = self.agent.run(nl)\n",
    "            return res\n",
    "        except Exception as e:\n",
    "            return f\"Error running NL->SQL agent: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILTumN696XXC"
   },
   "source": [
    "Analytics Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "lHtJznMk4fnV"
   },
   "outputs": [],
   "source": [
    "class AnalyticsEngine:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def compute_kpis(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        kpis = {}\n",
    "        if \"revenue\" in df.columns:\n",
    "            kpis[\"total_revenue\"] = float(df[\"revenue\"].sum())\n",
    "            kpis[\"avg_revenue\"] = float(df[\"revenue\"].mean())\n",
    "        if \"user_id\" in df.columns:\n",
    "            kpis[\"unique_users\"] = int(df[\"user_id\"].nunique())\n",
    "        return kpis\n",
    "\n",
    "    def trend_and_decompose(self, df: pd.DataFrame, date_col: str, value_col: str, freq: str = \"D\"):\n",
    "        df2 = df.copy()\n",
    "        df2[date_col] = pd.to_datetime(df2[date_col])\n",
    "        ts = df2.set_index(date_col).resample(freq).sum()[value_col].fillna(0)\n",
    "        # decomposition\n",
    "        decomposition = seasonal_decompose(ts, period=7, model=\"additive\", extrapolate_trend='freq')\n",
    "        # make plotly figure\n",
    "        fig = px.line(ts.reset_index(), x=date_col, y=value_col, title=f\"{value_col} trend\")\n",
    "        return {\n",
    "            \"decomposition\": {\n",
    "                \"trend\": decomposition.trend.dropna().to_dict(),\n",
    "                \"seasonal\": decomposition.seasonal.dropna().to_dict(),\n",
    "                \"resid\": decomposition.resid.dropna().to_dict()\n",
    "            },\n",
    "            \"fig_json\": fig.to_json()\n",
    "        }\n",
    "\n",
    "    def detect_anomalies_zscore(self, series: pd.Series, z_thresh: float = 3.0) -> List[Dict[str, Any]]:\n",
    "        mean = series.mean()\n",
    "        std = series.std(ddof=0)\n",
    "        z = (series - mean) / (std + 1e-9)\n",
    "        anomalies = series[abs(z) > z_thresh]\n",
    "        return [{\"index\": str(idx), \"value\": float(val), \"zscore\": float(z.loc[idx])} for idx, val in anomalies.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsSpdNQG6wRG"
   },
   "source": [
    "Demo app wiring (FastAPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "4Vr2iT4w4frp"
   },
   "outputs": [],
   "source": [
    "app = FastAPI(title=\"Advanced RAG + Analytics Prototype\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zJcv48s6zMv"
   },
   "source": [
    " Global in-memory objects (for demo / POC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "szZ5QyFf4fu9"
   },
   "outputs": [],
   "source": [
    "EMB_CLIENT = EmbeddingClient()\n",
    "DIM = 1536  # typical for many embed models; we'll infer later\n",
    "FAISS_STORE: Optional[FaissVectorStore] = None\n",
    "HYBRID_RETRIEVER: Optional[HybridRetriever] = None\n",
    "RAG_SERVICE: Optional[RAGService] = None\n",
    "SQL_AGENT: Optional[SQLAgentService] = None\n",
    "ANALYTICS = AnalyticsEngine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0i75wNAI67-g"
   },
   "source": [
    "For demo, create a temporary sqlite DB and seed with sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "12c1EFE76133"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akasar\\AppData\\Local\\Temp\\1\\ipykernel_17492\\4111426488.py:13: DeprecationWarning: \n",
      "        on_event is deprecated, use lifespan event handlers instead.\n",
      "\n",
      "        Read more about it in the\n",
      "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
      "        \n",
      "  @app.on_event(\"startup\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 116\u001b[39m\n\u001b[32m    114\u001b[39m demo_folder = seed_demo_docs()\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# start uvicorn server\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m \u001b[43muvicorn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrag_analytics_pipeline:app\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m0.0.0.0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreload\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\uvicorn\\main.py:580\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(app, host, port, uds, fd, loop, http, ws, ws_max_size, ws_max_queue, ws_ping_interval, ws_ping_timeout, ws_per_message_deflate, lifespan, interface, reload, reload_dirs, reload_includes, reload_excludes, reload_delay, workers, env_file, log_config, log_level, access_log, proxy_headers, server_header, date_header, forwarded_allow_ips, root_path, limit_concurrency, backlog, limit_max_requests, timeout_keep_alive, timeout_graceful_shutdown, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_version, ssl_cert_reqs, ssl_ca_certs, ssl_ciphers, headers, use_colors, app_dir, factory, h11_max_incomplete_event_size)\u001b[39m\n\u001b[32m    578\u001b[39m         Multiprocess(config, target=server.run, sockets=[sock]).run()\n\u001b[32m    579\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m580\u001b[39m         \u001b[43mserver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m    582\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# pragma: full coverage\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\uvicorn\\server.py:67\u001b[39m, in \u001b[36mServer.run\u001b[39m\u001b[34m(self, sockets)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, sockets: \u001b[38;5;28mlist\u001b[39m[socket.socket] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     66\u001b[39m     \u001b[38;5;28mself\u001b[39m.config.setup_event_loop()\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43msockets\u001b[49m\u001b[43m=\u001b[49m\u001b[43msockets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\runners.py:191\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(main, debug, loop_factory)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[33;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    187\u001b[39m \u001b[33;03m    asyncio.run(main())\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m events._get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    190\u001b[39m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    192\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug=debug, loop_factory=loop_factory) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m runner.run(main)\n",
      "\u001b[31mRuntimeError\u001b[39m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "def create_demo_sqlite(path=\"demo_data.db\"):\n",
    "    engine = create_engine(f\"sqlite:///{path}\")\n",
    "    df = pd.DataFrame({\n",
    "        \"date\": pd.date_range(\"2025-01-01\", periods=120, freq=\"D\"),\n",
    "        \"order_id\": [f\"ORD{1000+i}\" for i in range(120)],\n",
    "        \"user_id\": [f\"U{(i%10)}\" for i in range(120)],\n",
    "        \"product\": [\"A\" if i%3==0 else \"B\" if i%3==1 else \"C\" for i in range(120)],\n",
    "        \"revenue\": np.abs(np.random.randn(120).cumsum()) + 50\n",
    "    })\n",
    "    df.to_sql(\"sales\", engine, if_exists=\"replace\", index=False)\n",
    "    return engine\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "def startup_event():\n",
    "    global FAISS_STORE, HYBRID_RETRIEVER, RAG_SERVICE, SQL_AGENT, DIM, EMB_CLIENT\n",
    "    # set DIM by doing a dummy embedding of small text\n",
    "    try:\n",
    "        vec = EMB_CLIENT.embed_batch([\"test\"])\n",
    "        DIM = len(vec[0])\n",
    "        print(\"Embedding dim detected:\", DIM)\n",
    "    except Exception as e:\n",
    "        print(\"Warning: embedding call failed at startup:\", e)\n",
    "        DIM = 1536\n",
    "    FAISS_STORE = FaissVectorStore(dim=DIM, persist_dir=FAISS_INDEX_DIR, index_name=\"demo\")\n",
    "    HYBRID_RETRIEVER = HybridRetriever(FAISS_STORE, EMB_CLIENT)\n",
    "    RAG_SERVICE = RAGService(HYBRID_RETRIEVER)\n",
    "    engine = create_demo_sqlite()\n",
    "    SQL_AGENT = SQLAgentService(engine)\n",
    "    print(\"Startup complete. Services ready.\")\n",
    "\n",
    "# Pydantic models\n",
    "class UploadDocRequest(BaseModel):\n",
    "    doc_id: str\n",
    "    text: str\n",
    "\n",
    "class QueryRequest(BaseModel):\n",
    "    query: str\n",
    "    top_k: Optional[int] = 5\n",
    "\n",
    "class NLSQLRequest(BaseModel):\n",
    "    nl: str\n",
    "\n",
    "class AnalyticsRequest(BaseModel):\n",
    "    date_col: str\n",
    "    value_col: str\n",
    "    freq: Optional[str] = \"D\"\n",
    "\n",
    "# Endpoints\n",
    "@app.post(\"/upload_doc\")\n",
    "def upload_doc(req: UploadDocRequest):\n",
    "    \"\"\"Chunk, embed, and upsert to FAISS\"\"\"\n",
    "    global EMB_CLIENT, FAISS_STORE\n",
    "    chunks = create_chunks_from_doc(req.doc_id, req.text)\n",
    "    texts = [c.text for c in chunks]\n",
    "    meta = []\n",
    "    ids = []\n",
    "    try:\n",
    "        embeds = EMB_CLIENT.embed_batch(texts)\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Embedding failed: {e}\")\n",
    "\n",
    "    arr = np.array(embeds).astype(\"float32\")\n",
    "    for i, c in enumerate(chunks):\n",
    "        c.metadata[\"text\"] = c.text  # store the short text for cheap keyword scoring\n",
    "        ids.append(c.id)\n",
    "        meta.append(c.metadata)\n",
    "    # init vectorstore dim check\n",
    "    if FAISS_STORE.dim != arr.shape[1]:\n",
    "        print(\"WARNING: embedding dim mismatch, recreating FAISS store\")\n",
    "        FAISS_STORE = FaissVectorStore(dim=arr.shape[1], persist_dir=FAISS_INDEX_DIR, index_name=\"demo\")\n",
    "    FAISS_STORE.add(ids, arr, meta)\n",
    "    return {\"status\": \"ok\", \"n_chunks\": len(chunks)}\n",
    "\n",
    "@app.post(\"/rag_query\")\n",
    "def rag_query(req: QueryRequest):\n",
    "    global RAG_SERVICE\n",
    "    return RAG_SERVICE.answer(req.query, top_k=req.top_k)\n",
    "\n",
    "@app.post(\"/nl_sql\")\n",
    "def nl_sql(req: NLSQLRequest):\n",
    "    global SQL_AGENT\n",
    "    return {\"result\": SQL_AGENT.run_nl_query(req.nl)}\n",
    "\n",
    "@app.post(\"/analytics/run\")\n",
    "def run_analytics(req: AnalyticsRequest):\n",
    "    # For demo, read the sales table from demo sqlite and run analytics\n",
    "    engine = SQL_AGENT.engine\n",
    "    df = pd.read_sql(\"SELECT * FROM sales\", engine)\n",
    "    kpis = ANALYTICS.compute_kpis(df)\n",
    "    dec = ANALYTICS.trend_and_decompose(df, req.date_col, req.value_col, freq=req.freq)\n",
    "    # convert chart to JSON for client rendering\n",
    "    return {\"kpis\": kpis, \"decomposition\": dec[\"decomposition\"], \"fig_json\": dec[\"fig_json\"]}\n",
    "\n",
    "# Local run helper (seed docs)\n",
    "def seed_demo_docs():\n",
    "    demo_folder = Path(\"./demo_docs\")\n",
    "    demo_folder.mkdir(exist_ok=True)\n",
    "    # create a few sample long docs\n",
    "    doc1 = (\n",
    "        \"Billing policy: Refunds are allowed within 30 days of purchase with proof of purchase. \"\n",
    "        \"For subscription churn, prorated refunds are applied. The finance team must approve exceptions. \"\n",
    "    ) * 50\n",
    "    doc2 = (\n",
    "        \"Deployment notes: The ML model uses FAISS for vector search. Embeddings are generated via OpenAI. \"\n",
    "        \"Indexing pipeline runs nightly and stores metadata including doc_id and chunk_id. \"\n",
    "    ) * 50\n",
    "    (demo_folder / \"billing.txt\").write_text(doc1)\n",
    "    (demo_folder / \"infra.txt\").write_text(doc2)\n",
    "    return demo_folder\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # seed demo docs and upload them using the API programmatically\n",
    "    # Start server with uvicorn\n",
    "    demo_folder = seed_demo_docs()\n",
    "    # start uvicorn server\n",
    "    uvicorn.run(\"rag_analytics_pipeline:app\", host=\"0.0.0.0\", port=8000, reload=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "He8v0FQC616s"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yrM2T9tM619b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0F3GalYY62A_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgnTxRZk62D6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40S6MdEr62G4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Ua9GwvG62J7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
