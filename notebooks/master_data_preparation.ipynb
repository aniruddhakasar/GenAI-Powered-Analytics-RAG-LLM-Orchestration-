{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§  Master Data Preparation Notebook â€” Unified Cleaning, Normalization & Transformation\n",
    "This notebook integrates all datasets (Sales, Customer, Finance, Employee, Product, Supplier, RAG Docs, KPI) for preprocessing and model training preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data will be stored in: processed_data\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Imports & Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re, string\n",
    "\n",
    "# Define storage path\n",
    "BASE_PATH = 'processed_data'\n",
    "os.makedirs(BASE_PATH, exist_ok=True)\n",
    "print('Processed data will be stored in:', BASE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All datasets loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Load all datasets\n",
    "sales = pd.read_csv('D:/Desktop/GenAI-Powered Analytics Platform/Data/raw/sales_data.csv', parse_dates=['order_date'])\n",
    "customers = pd.read_csv('D:/Desktop/GenAI-Powered Analytics Platform/Data/raw/customer_data.csv', parse_dates=['signup_date','last_purchase_date'])\n",
    "finance = pd.read_csv('D:/Desktop/GenAI-Powered Analytics Platform/Data/raw/finance_data.csv', parse_dates=['transaction_date'])\n",
    "employees = pd.read_csv('D:/Desktop/GenAI-Powered Analytics Platform/Data/raw/employee_data.csv', parse_dates=['joining_date'])\n",
    "products = pd.read_csv('D:/Desktop/GenAI-Powered Analytics Platform/Data/raw/product_data.csv', parse_dates=['launch_date'])\n",
    "suppliers = pd.read_csv('D:/Desktop/GenAI-Powered Analytics Platform/Data/raw/supplier_data.csv', parse_dates=['contract_start','contract_end'])\n",
    "documents = pd.read_csv('D:/Desktop/GenAI-Powered Analytics Platform/Data/raw/business_documents_rag.csv', parse_dates=['created_date'])\n",
    "kpi = pd.read_csv('D:/Desktop/GenAI-Powered Analytics Platform/Data/raw/daily_kpi_data.csv', parse_dates=['date'])\n",
    "print('âœ… All datasets loaded successfully.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Missing values handled and duplicates removed.\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: Cleaning â€” Missing values & duplicates\n",
    "def clean_df(df):\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.replace(['None','nan','NaN','NULL','null','?',''], np.nan)\n",
    "    imputer = SimpleImputer(strategy='most_frequent')\n",
    "    df[:] = imputer.fit_transform(df)\n",
    "    return df\n",
    "\n",
    "sales = clean_df(sales)\n",
    "customers = clean_df(customers)\n",
    "finance = clean_df(finance)\n",
    "employees = clean_df(employees)\n",
    "products = clean_df(products)\n",
    "suppliers = clean_df(suppliers)\n",
    "documents = clean_df(documents)\n",
    "kpi = clean_df(kpi)\n",
    "print('âœ… Missing values handled and duplicates removed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Numerical normalization completed.\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: Data Normalization / Scaling (for numerical columns)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "def scale_numerical(df):\n",
    "    num_cols = df.select_dtypes(include=['float64','int64']).columns\n",
    "    df[num_cols] = scaler.fit_transform(df[num_cols])\n",
    "    return df\n",
    "\n",
    "sales = scale_numerical(sales)\n",
    "customers = scale_numerical(customers)\n",
    "finance = scale_numerical(finance)\n",
    "employees = scale_numerical(employees)\n",
    "products = scale_numerical(products)\n",
    "suppliers = scale_numerical(suppliers)\n",
    "kpi = scale_numerical(kpi)\n",
    "print('âœ… Numerical normalization completed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Categorical encoding completed.\n"
     ]
    }
   ],
   "source": [
    "# STEP 5: Encoding categorical variables\n",
    "le = LabelEncoder()\n",
    "\n",
    "def encode_categorical(df):\n",
    "    cat_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in cat_cols:\n",
    "        try:\n",
    "            df[col] = le.fit_transform(df[col].astype(str))\n",
    "        except:\n",
    "            pass\n",
    "    return df\n",
    "\n",
    "sales = encode_categorical(sales)\n",
    "customers = encode_categorical(customers)\n",
    "finance = encode_categorical(finance)\n",
    "employees = encode_categorical(employees)\n",
    "products = encode_categorical(products)\n",
    "suppliers = encode_categorical(suppliers)\n",
    "documents = encode_categorical(documents)\n",
    "kpi = encode_categorical(kpi)\n",
    "print('âœ… Categorical encoding completed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Date-based features created.\n"
     ]
    }
   ],
   "source": [
    "# STEP 6: Feature engineering examples (date parts)\n",
    "def add_date_features(df, date_col):\n",
    "    if date_col in df.columns:\n",
    "        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "        df[date_col + '_year'] = df[date_col].dt.year\n",
    "        df[date_col + '_month'] = df[date_col].dt.month\n",
    "        df[date_col + '_day'] = df[date_col].dt.day\n",
    "        df[date_col + '_weekday'] = df[date_col].dt.weekday\n",
    "    return df\n",
    "\n",
    "sales = add_date_features(sales, 'order_date')\n",
    "finance = add_date_features(finance, 'transaction_date')\n",
    "employees = add_date_features(employees, 'joining_date')\n",
    "products = add_date_features(products, 'launch_date')\n",
    "suppliers = add_date_features(suppliers, 'contract_end')\n",
    "kpi = add_date_features(kpi, 'date')\n",
    "print('âœ… Date-based features created.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'region_sale'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\1\\ipykernel_21976\\1275580158.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# STEP 7: Merge examples (to build a master dataset for ML)\u001b[39;00m\n\u001b[32m      2\u001b[39m master = sales.merge(customers, on='customer_id', how='left')\\\n\u001b[32m      3\u001b[39m                .merge(products, on='product_id', how='left', suffixes=('_sale','_prod'))\\\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m                .merge(finance, left_on=\u001b[33m'region_sale'\u001b[39m, right_on=\u001b[33m'department'\u001b[39m, how=\u001b[33m'left'\u001b[39m)\n\u001b[32m      5\u001b[39m \n\u001b[32m      6\u001b[39m print(\u001b[33m'Merged master dataset shape:'\u001b[39m, master.shape)\n\u001b[32m      7\u001b[39m master.head()\n",
      "\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m  10835\u001b[39m         validate: MergeValidate | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m  10836\u001b[39m     ) -> DataFrame:\n\u001b[32m  10837\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m pandas.core.reshape.merge \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[32m  10838\u001b[39m \n\u001b[32m> \u001b[39m\u001b[32m10839\u001b[39m         return merge(\n\u001b[32m  10840\u001b[39m             self,\n\u001b[32m  10841\u001b[39m             right,\n\u001b[32m  10842\u001b[39m             how=how,\n",
      "\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    166\u001b[39m             validate=validate,\n\u001b[32m    167\u001b[39m             copy=copy,\n\u001b[32m    168\u001b[39m         )\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m         op = _MergeOperation(\n\u001b[32m    171\u001b[39m             left_df,\n\u001b[32m    172\u001b[39m             right_df,\n\u001b[32m    173\u001b[39m             how=how,\n",
      "\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[39m\n\u001b[32m    790\u001b[39m             self.right_join_keys,\n\u001b[32m    791\u001b[39m             self.join_names,\n\u001b[32m    792\u001b[39m             left_drop,\n\u001b[32m    793\u001b[39m             right_drop,\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m         ) = self._get_merge_keys()\n\u001b[32m    795\u001b[39m \n\u001b[32m    796\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m left_drop:\n\u001b[32m    797\u001b[39m             self.left = self.left._drop_labels_or_levels(left_drop)\n",
      "\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1306\u001b[39m                     \u001b[38;5;28;01mif\u001b[39;00m lk \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1307\u001b[39m                         \u001b[38;5;66;03m# Then we're either Hashable or a wrong-length arraylike,\u001b[39;00m\n\u001b[32m   1308\u001b[39m                         \u001b[38;5;66;03m#  the latter of which will raise\u001b[39;00m\n\u001b[32m   1309\u001b[39m                         lk = cast(Hashable, lk)\n\u001b[32m-> \u001b[39m\u001b[32m1310\u001b[39m                         left_keys.append(left._get_label_or_level_values(lk))\n\u001b[32m   1311\u001b[39m                         join_names.append(lk)\n\u001b[32m   1312\u001b[39m                     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1313\u001b[39m                         \u001b[38;5;66;03m# work-around for merge_asof(left_index=True)\u001b[39;00m\n",
      "\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1907\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1908\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1909\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1910\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1912\u001b[39m \n\u001b[32m   1913\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1914\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'region_sale'"
     ]
    }
   ],
   "source": [
    "# STEP 7: Merge examples (to build a master dataset)\n",
    "master = sales.merge(customers, on='customer_id', how='left')\\\n",
    "               .merge(products, on='product_id', how='left', suffixes=('_sale','_prod'))\\\n",
    "               .merge(finance, left_on='region_sale', right_on='department', how='left')\n",
    "\n",
    "print('Merged master dataset shape:', master.shape)\n",
    "master.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All cleaned datasets saved in: processed_data\n"
     ]
    }
   ],
   "source": [
    "# STEP 8: Save processed datasets\n",
    "sales.to_csv(f'{BASE_PATH}/sales_cleaned.csv', index=False)\n",
    "customers.to_csv(f'{BASE_PATH}/customer_cleaned.csv', index=False)\n",
    "finance.to_csv(f'{BASE_PATH}/finance_cleaned.csv', index=False)\n",
    "employees.to_csv(f'{BASE_PATH}/employee_cleaned.csv', index=False)\n",
    "products.to_csv(f'{BASE_PATH}/product_cleaned.csv', index=False)\n",
    "suppliers.to_csv(f'{BASE_PATH}/supplier_cleaned.csv', index=False)\n",
    "documents.to_csv(f'{BASE_PATH}/documents_cleaned.csv', index=False)\n",
    "kpi.to_csv(f'{BASE_PATH}/kpi_cleaned.csv', index=False)\n",
    "#master.to_csv(f'{BASE_PATH}/master_dataset.csv', index=False)\n",
    "print('âœ… All cleaned datasets saved in:', BASE_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Ready for Model Training\n",
    "- The processed data in `/processed_data/` can now be used for supervised learning (classification/regression) or unsupervised modeling (clustering, segmentation).\n",
    "- Extend feature generation for domain-specific model inputs (e.g., KPI lag features, text embeddings, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
